{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "850f45df-665f-4208-82c3-00ef73a9bcbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Users/karthikeyansubha42@gmail.com/spark_streaming/work_count_program/workcount_application/Workcount_Cleanup_Activity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "093bc149-0410-42d8-b5a0-3d2e12d47b86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Users/karthikeyansubha42@gmail.com/spark_streaming/work_count_program/workcount_application/Workcount_Program/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84d28e4a-9ae4-4ec9-b21b-3cf1218655c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Do you want to drop the delta table (y/n): y"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta table drop successfully\nRunning Testcase 1:\nCleanUp working dir is started...Done.\nBatch load for Word Count is started...Done\nTestcase 1 Passed\n\nRunning Testcase 2:\nCleanUp working dir is started...Done.\nBatch load for Word Count is started...Done\nTestcase 2 Passed\n\nRunning Testcase 3:\nCleanUp working dir is started...Done.\nBatch load for Word Count is started...Done\nTestcase 3 Passed\n\n"
     ]
    }
   ],
   "source": [
    "class BatchLoadTestSuite(Batch_WordCount):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    \n",
    "    def test_cases(self):\n",
    "        self.dropping_delta_table()\n",
    "\n",
    "        print('Running Testcase 1:')\n",
    "        correct_value = 5\n",
    "        self.delete_and_recreating_dir()\n",
    "        self.data_injection(file_name='file_1.txt')\n",
    "        self.launcher()\n",
    "        val = spark.sql(\"select sum(count) from batch_wordcount_table where characters=='a' group by characters\").collect()[0][0]\n",
    "        assert correct_value==val, f'Tescase 1 got failed, your value is {val}'\n",
    "        print('Testcase 1 Passed\\n')\n",
    "\n",
    "        print('Running Testcase 2:')\n",
    "        correct_value = 5\n",
    "        self.delete_and_recreating_dir()\n",
    "        self.data_injection(file_name='file_2.txt')\n",
    "        self.launcher()\n",
    "        val = spark.sql(\"select sum(count) from batch_wordcount_table where characters=='b'  group by characters\").collect()[0][0]\n",
    "        assert correct_value==val, f'Tescase 2 got failed, your value is {val}'\n",
    "        print('Testcase 2 Passed\\n')\n",
    "\n",
    "        print('Running Testcase 3:')\n",
    "        self.delete_and_recreating_dir()\n",
    "        self.data_injection(file_name='file_3.txt')\n",
    "        self.launcher()\n",
    "        val = spark.sql(\"select sum(count) from batch_wordcount_table where characters=='!'  group by characters\").collect()\n",
    "        assert len(val)==0, f'Tescase 3 got failed, your value is {len(val)}'\n",
    "        print('Testcase 3 Passed\\n')\n",
    "\n",
    "obj = BatchLoadTestSuite()\n",
    "obj.test_cases()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbfe91c8-4f69-480c-8db6-7d5701fc46e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Do you want to drop the delta table (y/n): y"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta table drop successfully\nCleanUp working dir is started...Done.\nStream load for Word Count is started:\n\tRunning Testcase 1:Passed\n\tRunning Testcase 2:Passed\n\tRunning Testcase 3:Passed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "class StreamLoadTestSuite(Streaming_WordCount):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    \n",
    "    def test_cases(self):\n",
    "        self.dropping_delta_table()\n",
    "        self.delete_and_recreating_dir()\n",
    "        squery = self.launcher()\n",
    "\n",
    "        print('\\tRunning Testcase 1:', end='')\n",
    "        correct_value = 5\n",
    "        self.data_injection(file_name='file_1.txt')\n",
    "        time.sleep(30)\n",
    "        val = spark.sql(\"select sum(count) from stream_wordcount_table where characters=='a' group by characters\").collect()[0][0]\n",
    "        assert correct_value==val, f'Tescase 1 got failed, your value is {val}'\n",
    "        print('Passed')\n",
    "\n",
    "        print('\\tRunning Testcase 2:', end='')\n",
    "        correct_value = 6\n",
    "        self.data_injection(file_name='file_2.txt')\n",
    "        time.sleep(30)\n",
    "        val = spark.sql(\"select sum(count) from stream_wordcount_table where characters=='b'  group by characters\").collect()[0][0]\n",
    "        assert correct_value==val, f'Tescase 2 got failed, your value is {val}'\n",
    "        print('Passed')\n",
    "\n",
    "        print('\\tRunning Testcase 3:', end='')\n",
    "        self.data_injection(file_name='file_3.txt')\n",
    "        time.sleep(30)\n",
    "        val = spark.sql(\"select sum(count) from stream_wordcount_table where characters=='!'  group by characters\").collect()\n",
    "        assert len(val)==0, f'Tescase 3 got failed, your value is {len(val)}'\n",
    "        print('Passed')\n",
    "\n",
    "        squery.stop()\n",
    "\n",
    "obj = StreamLoadTestSuite()\n",
    "obj.test_cases()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3440731142577987,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "WordCount_Test",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
