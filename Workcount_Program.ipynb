{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d54f0e99-c160-4d91-b11b-368978f7d71b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode, lower, trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2553215-0e15-4d82-b6f9-8a2e6f8d3eea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4408937087253071>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mBatch_WordCount\u001B[39;00m(\u001B[43mcleanup_activity\u001B[49m):\n",
       "\u001B[1;32m      3\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n",
       "\u001B[1;32m      4\u001B[0m         \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'cleanup_activity' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-4408937087253071>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mBatch_WordCount\u001B[39;00m(\u001B[43mcleanup_activity\u001B[49m):\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m      4\u001B[0m         \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\n\u001B[0;31mNameError\u001B[0m: name 'cleanup_activity' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'cleanup_activity' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Batch_WordCount(cleanup_activity):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def get_raw_data(self):\n",
    "        self.df = spark.read.option('lineSep','.').text(f'{self.base_dir}/working')\n",
    "        return self.df.withColumn('characters', explode(split(self.df.value,'')))\n",
    "    \n",
    "    def get_quality_data(self, Rawdf):\n",
    "        return (Rawdf.select(Rawdf.value.alias('Text'), lower(trim(Rawdf.characters)).alias('characters'))\\\n",
    "            .where(Rawdf['characters'].isNotNull())\\\n",
    "            .where(Rawdf.characters.rlike(\"[a-zA-Z]\")))\n",
    "    \n",
    "    def get_word_count(self, Quadf):\n",
    "        return Quadf.groupBy(Quadf.Text, Quadf.characters).count()\n",
    "    \n",
    "    def inserting_into_table(self, Aggdf):\n",
    "        Aggdf.write.format('delta')\\\n",
    "                    .mode('overwrite')\\\n",
    "                    .saveAsTable('batch_wordcount_table')\n",
    "        \n",
    "    \n",
    "    def launcher(self):\n",
    "        print('Batch load for Word Count is started...', end='')\n",
    "        Rawdf = self.get_raw_data()\n",
    "        Quadf = self.get_quality_data(Rawdf)\n",
    "        Aggdf = self.get_word_count(Quadf)\n",
    "        self.inserting_into_table(Aggdf)\n",
    "        print('Done')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Workcount_Program",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
