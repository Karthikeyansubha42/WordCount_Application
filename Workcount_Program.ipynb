{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d54f0e99-c160-4d91-b11b-368978f7d71b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode, lower, trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2553215-0e15-4d82-b6f9-8a2e6f8d3eea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Batch_WordCount(cleanup_activity):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def get_raw_data(self):\n",
    "        self.df = spark.read.option('lineSep','.').text(f'{self.base_dir}/working')\n",
    "        return self.df.withColumn('characters', explode(split(self.df.value,'')))\n",
    "    \n",
    "    def get_quality_data(self, Rawdf):\n",
    "        return (Rawdf.select(Rawdf.value.alias('Text'), lower(trim(Rawdf.characters)).alias('characters'))\\\n",
    "            .where(Rawdf['characters'].isNotNull())\\\n",
    "            .where(Rawdf.characters.rlike(\"[a-zA-Z]\")))\n",
    "    \n",
    "    def get_word_count(self, Quadf):\n",
    "        return Quadf.groupBy(Quadf.Text, Quadf.characters).count()\n",
    "    \n",
    "    def inserting_into_table(self, Aggdf):\n",
    "        Aggdf.write.format('delta')\\\n",
    "                    .mode('overwrite')\\\n",
    "                    .saveAsTable('batch_wordcount_table')\n",
    "        \n",
    "    \n",
    "    def launcher(self):\n",
    "        print('Batch load for Word Count is started...', end='')\n",
    "        Rawdf = self.get_raw_data()\n",
    "        Quadf = self.get_quality_data(Rawdf)\n",
    "        Aggdf = self.get_word_count(Quadf)\n",
    "        self.inserting_into_table(Aggdf)\n",
    "        print('Done')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34aff4ca-2ecf-4aca-bddb-9bba55154e61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Streaming_WordCount(cleanup_activity):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def get_raw_data(self):\n",
    "        self.df = spark.readStream.option('lineSep','.').text(f'{self.base_dir}/working')\n",
    "        return self.df.withColumn('characters', explode(split(self.df.value,'')))\n",
    "    \n",
    "    def get_quality_data(self, Rawdf):\n",
    "        return (Rawdf.select(Rawdf.value.alias('Text'), lower(trim(Rawdf.characters)).alias('characters'))\\\n",
    "            .where(Rawdf['characters'].isNotNull())\\\n",
    "            .where(Rawdf.characters.rlike(\"[a-zA-Z]\")))\n",
    "    \n",
    "    def get_word_count(self, Quadf):\n",
    "        return Quadf.groupBy(Quadf.Text, Quadf.characters).count()\n",
    "    \n",
    "    def inserting_into_table(self, Aggdf):\n",
    "        return (Aggdf.writeStream.format('delta')\\\n",
    "                    .option('checkpointLocation', f'{self.base_dir}/checkpoint')\n",
    "                    .outputMode('complete')\\\n",
    "                    .toTable('stream_wordcount_table'))\n",
    "        \n",
    "    \n",
    "    def launcher(self):\n",
    "        print('Stream load for Word Count is started:')\n",
    "        Rawdf = self.get_raw_data()\n",
    "        Quadf = self.get_quality_data(Rawdf)\n",
    "        Aggdf = self.get_word_count(Quadf)\n",
    "        squery = self.inserting_into_table(Aggdf)\n",
    "        return squery\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Workcount_Program",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
